---
title: 'DepGraph: Towards Any Structural Pruning'
date: 2024-05-13 14:19:41
auther: 小狼
summary: 介绍Torch-Pruning结构化剪枝库及其底层算法DepGraph
categories: 论文
img: /medias/featureimages/3.jpg
tags:
  - LLM
  - 加速
---

# DepGraph: Towards Any Structural Pruning

## 概要

结构化剪枝通过从神经网络中**删除结构化分组的参数**来实现模型加速。然而，参数分组模式在不同的模型中存在很大差异，这使得特定于体系结构的修剪器**依赖于手工设计的分组方案，无法推广到新的体系结构中**。

本工作在结构化剪枝的**自动化上做了改进**，提出了一种非深度图算法DepGraph，实现了**架构通用的结构化剪枝**，适用于CNNs,，Transformers, RNNs, GNNs等网络。DepGraph能够显式地建模层之间的依赖关系，并对耦合参数进行综合剪枝。自动地分析复杂的结构耦合，从而正确地移除参数实现网络加速。基于DepGraph算法，作者开发了PyTorch结构化剪枝框架 Torch-Pruning。不同于依赖Masking实现的“模拟剪枝”，该框架能够**实际地移除参数和通道，降低模型推理成本**。在DepGraph的帮助下，研究者和工程师无需再与复杂的网络结构斗智斗勇，可以轻松完成复杂模型的一键剪枝。

本文在几个架构和任务包括ResNe (X)t，DenseNet，MobileNet，VIT，GAT，DGCNN，LSTM上进行广泛评估。并证明提出的方法始终产生令人满意的性能。

## 引言

边缘计算应用需要深度神经网络的压缩。在众多的网络压缩范式中，剪枝已经被证明是高效和实用的。网络剪枝的目标是**从给定的网络中去除冗余参数，以使模型更轻量化，并潜在地加快推理速度。**主流剪枝方法大致可分为两类：结构化剪枝和非结构化剪枝。

* 结构化剪枝：通过从物理上去除分组参数来改变神经网络的结构；
* 非结构化剪枝：对部分权值进行调零，而不修改网络结构。

在实践中非结构化剪枝能够直接地实现并且天然适用于各种网络，但它通常需要专门的人工智能加速器或软件来实现模型加速。而结构化剪枝不依赖于特定的人工智能加速器或软件来减少内存消耗和计算成本，应用更广泛。

挑战1：

深度神经网络建立在大量的基本模块之上，如卷积、标准化或激活，然而这些模块，无论参数化或未参数化，都是通过复杂的连接内在耦合的。因此，即**使我们试图从CNN中只删除一个channel（如图1a），我们必须同时处理它对所有层的相互依赖关系**。（这张图中，残差连接需要两个conv的参数来共享channel，所以修改conv2需要同时修改conv1，BN1，BN2）

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240513144816132.png" style="zoom:80%;" >

（*图1：来自不同层的参数在跨网络架构中本质上是相互依赖的，这迫使多个层必须同时被修剪。本文引入了一个通用的方案，称为依赖图，以显式地解释这种依赖，并以全自动的方式对任意架构执行修剪。*）

* 依赖性不仅出现在残差结构中，在现代模型中可能是无限复杂的。现有的结构化剪枝方案都依赖于个案分析，也就是**针对于特定的网络**。虽然效果好但费时费力且不能推广。
* 本文模型为了跟踪不同层之间的依赖关系，**将依赖链分解并建模为一个递归过程**，这自然可以归结为**在图中寻找最大连通分量**的问题，并且可以通过图遍历来实现O (N)复杂度。具体来说，对于网络中要修剪的层，可以将其作为根来触发相邻耦合层上的修剪，然后继续以被触发层为起点递归重复触发过程。通过这样做，可以全面收集所有耦合层以进行修剪。

挑战2：

在结构剪枝中，分组层同时被修剪，这期望**同一组中所有被删除的参数都是不重要的**。这样的话，由于和其他层之间的关联，在单层中的参数重要性就不能反映真实情况。在不同的层上估计的重要性很可能是非加性的，有时甚至是相互矛盾的，这使得很难选择真正不重要的组来进行修剪。

为了解决这个问题，本文利用DepGraph的依赖建模能力设计了一个“分组级别”的重要性标准，该准则学习组内的一致稀疏性，以便那些 可以安全地删除那些归零的层，而不会造成太多的性能损失。通过依赖建模，在实验中表明了一个简单的L1范数准则可以达到与现代方法相当的性能。

总之，本文的贡献是针对任何结构修剪的通用修剪方案，称为依赖图（DepGraph），它允许自动参数分组，并有效地提高了各种网络架构（包括CNN、RNN、GNN和Vision Transformer）上结构修剪的可推广性。

## 相关工作

* 剪枝：剪枝算法的设计空间包括一系列方面，包括**剪枝方案、参数选择、层稀疏性和训练技术**。近年来，人们引入了许多稳健的标准，如基于幅度的标准和基于梯度的标准。另一种类型的方法通过稀疏训练来区分不重要的参数，稀疏训练将一些参数推到零以进行修剪。与那些静态标准相比，稀疏训练更可能找到不重要的参数，但由于需要网络训练，因此需要更多的计算资源。最近，还进行了一项综合性研究，以评估各种标准的效果，并提供一个公平的基准。

* 修剪分组参数：依赖性建模是任何结构修剪的关键和前提步骤，因为它涉及同时删除由于复杂的网络架构而在结构上彼此耦合的参数。剪枝分组参数的概念从结构剪枝的早期就已被研究。例如，**当修剪两个连续卷积层时，修剪第一层内的卷积核会导致在后续层中去除与该滤波器相关的核**。最近，已经提出了一些试点工作来解决层之间的复杂关系，并利用分组属性来提高结构修剪性能。

  不幸的是，现有的技术仍然依赖于经验规则或预定义的架构模式，这使得它们在所有结构剪枝应用程序中都不够通用。在本研究中，我们提出了一种解决这一挑战的通用方法，证明了解决参数依赖性有效地推广了广泛网络的结构剪枝，从而在多个任务上获得令人满意的性能。

## 本文方法

### 神经网络的依赖性

不失一般性，在全连接层开发本文方法。从由三个连续层组成的线性神经网络开始，如图2 (a)所示，分别由二维**权重矩阵**wl、wl+1和wl+2参数化。这种简单的神经网络可以通过**去除神经元的结构**修剪而变得轻量化。在这种情况下，很容易发现参数之间存在一些依赖关系，表示为wl⇔wl+1, 这**迫使二者同时被修剪**。具体而言，为了修剪连接wl和wl+1的第k个神经元，将移除wl [k, :]和wl+1[:, k]。

> 当我们希望通过剪枝某个神经元（高亮表示）实现加速时，与该神经元相连的多组参数需要被同时移除，**这些参数就组成了结构化剪枝的最小单元，通常称为组（Group）**。然而，在不同的网络架构中，参数的分组方式通常千差万别。图2（b）-（d）分别可视化了残差结构、拼接结构、以及降维度结构所致的参数分组情况，这些结构甚至可以互相嵌套，从而产生更加复杂的分组模式。

通过手动设计和模型特定的方案来处理层依赖性，以逐个案例的方式手动分析所有这些依赖关系是很难的，更不用说简单的依赖关系可以嵌套或组合成更任意复杂的模式。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240513160026964.png">

（*图2：在不同的结构中具有相互依赖性的分组参数。必须同时修剪所有突出显示的参数。*）

为了解决结构修剪中的依赖问题，在本文工作中引入了依赖关系图，它为依赖关系建模提供了一种通用的、全自动的机制。

### 依赖图

#### 分组

为了实现结构化剪枝，首先需要根据层之间的相互依赖关系进行分组。形式上，目标是找到一个分组矩阵G∈R^L*L，其中L是一个待修剪网络的深度，Gij=1表示第i层和第j层之间存在依赖关系。令Diag(G)=1^1×L来保证自依赖。使用分组矩阵，很容易**找到所有与第i层相互依赖的耦合层**，即找到组：

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240513161232441.png">

然而，由于现代深度网络可能由**数千层复杂的连接**组成，从神经网络中估计分组模式并不简单，他可能产生一个**大而复杂的分组矩阵G**。在这个矩阵中，Gij不仅由第i层和第j层决定，而且还受到它们之间的中间层的影响。这种非局部隐式关系在大多数情况下都不能用简单的规则来处理。为了克服这一挑战，作者没有直接估计分组矩阵G，而是**提出了一种等效但易于估计的依赖建模方法，即依赖图，从中可以有效地推导出G**。

#### 依赖图

考虑一个分组g = {w1, w2, w3}，其中存在依赖关系w1⇔w2，w2⇔w3和1⇔w3。可以观察到**冗余依赖**（例如w1到w3可以通过w1w2，w2w3来推出传递依赖）。首先，我们以w1为起点，并检查它对其他层的依赖性，例如w1⇔w2。此时w2作为新的起点以递归扩展依赖，触发w2⇔w3，**这个递归过程最终以一个传递关系结束**，w1⇔w2⇔w3。在这种情况下，我们只需要两个依赖关系来描述组g中的关系。类似地，第3.2节中讨论的分组矩阵对于依赖关系建模也是冗余的，因此可以在保留相同信息的同时，压缩成更少的边和更紧凑的形式。

**一种新的图D测量相邻层之间的局部相互依赖性，称为依赖图，可以作为分组矩阵G的有效约简。**

**依赖图只记录具有直接连接的相邻层之间的依赖关系。**他具有和G一样的顶点，但是有尽可能少的边。形式上，D被构造为，对于所有的Gij = 1，在顶点i和j之间存在一条路径。因此，Gij可以通过检验D中顶点i和j之间的路径的存在来得到。

#### 网络分解

在层级之间构建依赖图在实践中可能存在问题。一些基本层，如全连接层，可能有两种不同的修剪方案，如w[k, : ]和w[ : , k]，它们**分别压缩输入和输出的维度**（如前面图2所示，同样的全连接层，要考虑两种剪枝）。此外，网络还包含**非参数化的操作，如跳跃连接**，这也会影响层[40]之间的依赖性。

对于一个卷积层而言，我们可以对参数的不同维度进行独立的修剪，从而分别剪枝输入通道或者输出通道。然而，上述的依赖图D却无法对这一现象进行建模。为此，我们提出了一种更细粒度的模型描述符，**将网络F（x；w）分解为更精细、更基本的组件**，记为F = {f1，f2，...，fL}，其中每个组件f表示（1）一个参数层如卷积层 或者（2）一个非参数操作如残差连接。**关注层的输入和输出之间的关系而不是在层之间建模**。具体地说，将分量fi的输入和输出分别表示为𝑓𝑖−和𝑓𝑖+。对于任何网络，最终的分解都可以形式化为F = {𝑓1−，𝑓1+，...，𝑓L−，𝑓L+}。这种表示法更容易进行依赖关系建模，并允许**对同一层使用不同的剪枝方案。**

#### 依赖建模

利用这种符号，我们将神经网络重新定义为方程2，其中可以识别出两种主要的依赖类型，即层间依赖和层内依赖，如下所示：

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240516144322342.png" alt="image-20240516144322342" style="zoom:80%;" />

**符号↔表示两个相邻层之间的连通性。**对这两个依赖关系的检查产生了简单但通用的**依赖关系建模规则**：

* 层间依赖性：依赖性fi−⇔fj+持续出现在连接层中，其中𝑓𝑖−↔𝑓j+。由于一个层的输出和下一层的输入对应的是同一个中间特征（Feature），这就导致两者需要被同时剪枝。例如在通道剪枝中，“某一层的的输出通道剪枝”和“相邻后续层的输入通道剪枝”是等价的。

* 层内依赖性：在神经网络中，我们可以把各种层分为两类：第一类层的输入输出可以独立地进行剪枝，分别拥有不同的剪枝布局（pruning scheme），记作 𝑠𝑐ℎ(𝑓𝑖+) 或者 𝑠𝑐ℎ(𝑓𝑖−) 。例如对于全连接层的2D参数矩阵 𝑤 ，可以得到 𝑤[𝑘,:] 和 𝑤[:,𝑘] 两种不同的布局。这种情况下，输入 𝑓𝑖− 和输出 𝑓𝑖+ 在依赖图中是相互独立、非耦合的；而另一类层输入输出之间存在耦合，例如逐元素运算、Batch Normalization等。他们的参数（如果有）仅有一种剪枝布局，且同时影响输入输出的维度。如果 𝑓𝑖− 和 𝑓𝑖+ 共享相同的修剪方案，则存在依赖 𝑓𝑖− ⇔ 𝑓𝑖+ ，用𝑠𝑐ℎ(𝑓𝑖-) =𝑠𝑐ℎ(𝑓𝑖+) 表示。实际上，相比于复杂的参数耦合类型，深度网络中的层类型是非常有限的，我们可以预先定义不同层的剪枝布局来确定图中的依赖关系。

正式定义如下的依赖关系建模：

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240517110026498.png" alt="image-20240517110026498" style="zoom:80%;" />

其中 ∨ 和 ∧ 分别表示逻辑”OR“和“AND”。第一项检查由网络连接引起的层间依赖关系，而第二项检查由层输入和输出之间的共享剪枝方案所引入的层内依赖关系

我们在算法1和算法2中总结了依赖图构建和参数分组的过程，其中参数分组是一个递归的连通分量（Connected Component）搜索问题，可以通过简单深度(DFS)或者宽度(BFS)优先搜索实现。算法2简要描述了这一过程，即以某个节点i作为起始分组g，找到依赖图D中与之相连的新节点j，合并入当前组，直到不存在新的联通节点为止。此处省略了分组的去重处理。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240517112155305.png" alt="image-20240517112155305" style="zoom:80%;" />

将上述算法应用于一个具体的残差结构块，我们可以得到如下可视化结果。在具体剪枝时，以任意一个节点作为起始点，例如以 𝑓4+ 作为起点，递归地搜索能够访问到的所有其他节点，并将它们归入同一个组进行剪枝。值得注意的是，卷积网络由于输入输出使用了不同的剪枝布局（ 𝑠𝑐ℎ(𝑓4−)≠𝑠𝑐ℎ(𝑓4+) ），在依赖图中其输入输出节点间不存在层内依赖，但是由于skip连接 𝑓7 的存在，递归搜索过程中 𝑓4−和 𝑓4+ 会被分入同一组，即他们依旧需要被同时裁剪。其他层例如Batch Normalization的输入输出则存在简单的层内依赖。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240517105422995.png" alt="image-20240517105422995" style="zoom:80%;" />

图3。层分组是通过在DepGraph上递归传递实现的（从f4+开始）

### 组级别剪枝

依赖图的一个重要作用是参数自动分组，从而实现任意架构的模型剪枝。实际上，依赖图的自动分组能力还可以帮助设计**组级别剪枝（Group-level Pruning）**。在结构化剪枝中，属于同一组的参数会被同时移除，这一情况下需要保证这些被移除参数是“一致冗余”的，如果这些参数中包含对网络预测至关重要的参数，那么移除这些参数难免会损伤性能。

一个重要问题**是如何在涉及多个耦合层的情况下评估分组参数的重要性程度**。在本节中，作者利用一个简单的norm-based标准来建立一个实用的组级剪枝方法。给定一个参数组g = {w1，w2，...，w|g|}，现有的标准如L2-norm重要性可以对每一个w产生独立的分数。估计组重要性的一种自然方法是计算一个聚合分数I(g)。但独立估计的各层重要性程度是非加性的，且由于分布和大小的散度而毫无意义。

为了使这个简单的聚合能够作用于重要性估计，我们**提出了一种稀疏训练方法来在组级别上稀疏参数**（如图4(c)），这些零化的组就可以安全地从网络中移除。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524101302850.png" alt="image-20240524101302850" style="zoom:80%;" />

如图4 (c)所示，目标是学习所有分组层之间的一致稀疏性，同时将某些维度归零为零。作者将分组参数扁平化并合并为一个大的参数矩阵，其中检索所有属于第k个可调维数的参数，就像CNN块的第k个通道一样。现在，一致的稀疏性可以通过一个简单的加权收缩来促进（采用一个L2正则项，通过赋予参数组的不同正则权重 𝛾 来进行组稀疏化）

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524104156855.png" alt="image-20240524104156855" style="zoom:80%;" />

其中k用于**可剪枝参数的切片**（Slicing），用于定位当前参数内**第k组参数子矩阵**，上述稀疏算法会得到**k组不同程度稀疏的耦合参数**，我们选择整体L2 norm最小的耦合参数进行剪枝。我们使用一个可控的指数策略来确定γk如下：

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524104920144.png" alt="image-20240524104920144" style="zoom:80%;" />

经过稀疏训练后，作者进一步使用了一个相对分数<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524105432943.png" alt="image-20240524105432943" style="zoom: 67%;" />来识别和删除不重要的参数。

实际上，依赖图还可以用于设计各种更强大的**组剪枝方法**，但由于稀疏训练、重要性评估等技术并非DepGraph的主要内容，这里也就不再赘述。

## 实验

### 设置

本文主要关注分类任务，在各种数据集上进行广泛的实验，如用于图像分类的CIFAR和ImageNet，用于图形分类的PPI，用于三维分类的ModelNet，用于文本分类的AGNews。使用模型如概要所述。为了进行ImageNet实验，使用了Torchvision 的现成模型作为原始模型。修剪后，所有模型都将按照与预训练阶段相似的协议进行微调，使用更小的学习率和更少的迭代。

### 性能

CIFAR是一个小型的图像数据集，被广泛用于验证剪枝算法的有效性。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524110639744.png" alt="image-20240524110639744" style="zoom:80%;" />

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524113933711.png" alt="image-20240524113933711"  />

### 分析

#### 一致性稀疏

在分析实验中，首先我们首先评估了一致性稀疏和逐层独立稀疏的差异，结论符合3.3中的分析，即逐层算法无法实现依赖参数的一致稀疏。例如下图中绿色的直方图表示传统的逐层稀疏策略，相比于本文提出的一致性稀疏，其整体稀疏性表现欠佳。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240524114128417.png" alt="image-20240524114128417" style="zoom:80%;" />

#### 分组策略



## 参考

https://zhuanlan.zhihu.com/p/619146631

https://developer.aliyun.com/article/1231617