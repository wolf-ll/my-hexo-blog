---
title: 'DepGraph: Towards Any Structural Pruning'
date: 2024-05-13 14:19:41
auther: 小狼
summary: 介绍Torch-Pruning结构化剪枝库及其底层算法DepGraph
categories: 论文
img: /medias/featureimages/3.jpg
tags:
  - LLM
  - 加速
---

# DepGraph: Towards Any Structural Pruning

## 概要

结构化剪枝通过从神经网络中**删除结构化分组的参数**来实现模型加速。然而，参数分组模式在不同的模型中存在很大差异，这使得特定于体系结构的修剪器**依赖于手工设计的分组方案，无法推广到新的体系结构中**。

本工作在结构化剪枝的**自动化上做了改进**，提出了一种非深度图算法DepGraph，实现了**架构通用的结构化剪枝**，适用于CNNs,，Transformers, RNNs, GNNs等网络。DepGraph能够显式地建模层之间的依赖关系，并对耦合参数进行综合剪枝。自动地分析复杂的结构耦合，从而正确地移除参数实现网络加速。基于DepGraph算法，作者开发了PyTorch结构化剪枝框架 Torch-Pruning。不同于依赖Masking实现的“模拟剪枝”，该框架能够**实际地移除参数和通道，降低模型推理成本**。在DepGraph的帮助下，研究者和工程师无需再与复杂的网络结构斗智斗勇，可以轻松完成复杂模型的一键剪枝。

本文在几个架构和任务包括ResNe (X)t，DenseNet，MobileNet，VIT，GAT，DGCNN，LSTM上进行广泛评估。并证明提出的方法始终产生令人满意的性能。

## 引言

边缘计算应用需要深度神经网络的压缩。在众多的网络压缩范式中，剪枝已经被证明是高效和实用的。网络剪枝的目标是**从给定的网络中去除冗余参数，以使模型更轻量化，并潜在地加快推理速度。**主流剪枝方法大致可分为两类：结构化剪枝和非结构化剪枝。

* 结构化剪枝：通过从物理上去除分组参数来改变神经网络的结构；
* 非结构化剪枝：对部分权值进行调零，而不修改网络结构。

在实践中非结构化剪枝能够直接地实现并且天然适用于各种网络，但它通常需要专门的人工智能加速器或软件来实现模型加速。而结构化剪枝不依赖于特定的人工智能加速器或软件来减少内存消耗和计算成本，应用更广泛。

挑战1：

深度神经网络建立在大量的基本模块之上，如卷积、标准化或激活，然而这些模块，无论参数化或未参数化，都是通过复杂的连接内在耦合的。因此，即**使我们试图从CNN中只删除一个channel（如图1a），我们必须同时处理它对所有层的相互依赖关系**。（这张图中，残差连接需要两个conv的参数来共享channel，所以修改conv2需要同时修改conv1，BN1，BN2）

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240513144816132.png" style="zoom:80%;" >

（*图1：来自不同层的参数在跨网络架构中本质上是相互依赖的，这迫使多个层必须同时被修剪。本文引入了一个通用的方案，称为依赖图，以显式地解释这种依赖，并以全自动的方式对任意架构执行修剪。*）

* 依赖性不仅出现在残差结构中，在现代模型中可能是无限复杂的。现有的结构化剪枝方案都依赖于个案分析，也就是**针对于特定的网络**。虽然效果好但费时费力且不能推广。
* 本文模型为了跟踪不同层之间的依赖关系，**将依赖链分解并建模为一个递归过程**，这自然可以归结为**在图中寻找最大连通分量**的问题，并且可以通过图遍历来实现O (N)复杂度。具体来说，对于网络中要修剪的层，可以将其作为根来触发相邻耦合层上的修剪，然后继续以被触发层为起点递归重复触发过程。通过这样做，可以全面收集所有耦合层以进行修剪。

挑战2：

在结构剪枝中，分组层同时被修剪，这期望**同一组中所有被删除的参数都是不重要的**。这样的话，由于和其他层之间的关联，在单层中的参数重要性就不能反映真实情况。在不同的层上估计的重要性很可能是非加性的，有时甚至是相互矛盾的，这使得很难选择真正不重要的组来进行修剪。

为了解决这个问题，本文利用DepGraph的依赖建模能力设计了一个“分组级别”的重要性标准，该准则学习组内的一致稀疏性，以便那些 可以安全地删除那些归零的层，而不会造成太多的性能损失。通过依赖建模，在实验中表明了一个简单的L1范数准则可以达到与现代方法相当的性能。

总之，本文的贡献是针对任何结构修剪的通用修剪方案，称为依赖图（DepGraph），它允许自动参数分组，并有效地提高了各种网络架构（包括CNN、RNN、GNN和Vision Transformer）上结构修剪的可推广性。

## 相关工作

* 剪枝：剪枝算法的设计空间包括一系列方面，包括**剪枝方案、参数选择、层稀疏性和训练技术**。近年来，人们引入了许多稳健的标准，如基于幅度的标准和基于梯度的标准。另一种类型的方法通过稀疏训练来区分不重要的参数，稀疏训练将一些参数推到零以进行修剪。与那些静态标准相比，稀疏训练更可能找到不重要的参数，但由于需要网络训练，因此需要更多的计算资源。最近，还进行了一项综合性研究，以评估各种标准的效果，并提供一个公平的基准。

* 修剪分组参数：依赖性建模是任何结构修剪的关键和前提步骤，因为它涉及同时删除由于复杂的网络架构而在结构上彼此耦合的参数。剪枝分组参数的概念从结构剪枝的早期就已被研究。例如，**当修剪两个连续卷积层时，修剪第一层内的卷积核会导致在后续层中去除与该滤波器相关的核**。最近，已经提出了一些试点工作来解决层之间的复杂关系，并利用分组属性来提高结构修剪性能。

  不幸的是，现有的技术仍然依赖于经验规则或预定义的架构模式，这使得它们在所有结构剪枝应用程序中都不够通用。在本研究中，我们提出了一种解决这一挑战的通用方法，证明了解决参数依赖性有效地推广了广泛网络的结构剪枝，从而在多个任务上获得令人满意的性能。

## 本文方法

### 神经网络的依赖性

不失一般性，在全连接层开发本文方法。从由三个连续层组成的线性神经网络开始，如图2 (a)所示，分别由二维权重矩阵wl、wl+1和wl+2参数化。这种简单的神经网络可以通过去除神经元的结构修剪而变得轻量化。在这种情况下，很容易发现参数之间存在一些依赖关系，表示为wl⇔wl+1, 这迫使二者同时被修剪。具体而言，为了修剪连接wl和wl+1的第k个神经元，将移除wl [k, :]和wl[:, k]。

通过手动设计和模型特定的方案来处理层依赖性，以逐个案例的方式手动分析所有这些依赖关系是很难的，更不用说简单的依赖关系可以嵌套或组合成更任意复杂的模式。

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240513160026964.png">

（*图2：在不同的结构中具有相互依赖性的分组参数。必须同时修剪所有突出显示的参数。*）

为了解决结构修剪中的依赖问题，在本文工作中引入了依赖关系图，它为依赖关系建模提供了一种通用的、全自动的机制。

### 依赖图

#### 分组

为了实现结构化剪枝，首先需要根据层之间的相互依赖关系进行分组。形式上，目标是找到一个分组矩阵G∈R^L*L，其中L是一个待修剪网络的深度，Gij=1表示第i层和第j层之间存在依赖关系。令Diag(G)=1^1×L来保证自依赖。使用分组矩阵，很容易**找到所有与第i层相互依赖的耦合层**，即找到组：

<img src="DepGraph-Towards-Any-Structural-Pruning\image-20240513161232441.png">

然而，由于现代深度网络可能由**数千层复杂的连接**组成，从神经网络中估计分组模式并不简单，他可能产生一个大而复杂的分组矩阵G。在这个矩阵中，Gij不仅由第i层和第j层决定，而且还受到它们之间的中间层的影响。这种非局部隐式关系在大多数情况下都不能用简单的规则来处理。为了克服这一挑战，作者没有直接估计分组矩阵G，而是**提出了一种等效但易于估计的依赖建模方法，即依赖图，从中可以有效地推导出G**。

#### 依赖图

考虑一个分组g = {w1, w2, w3}，其中存在依赖关系w1⇔w2，w2⇔w3和1⇔w3。可以观察到冗余依赖（例如w1到w3可以通过w1w2，w2w3来推出传递依赖）。首先，我们以w1为起点，并检查它对其他层的依赖性，例如w1⇔w2。此时w2作为新的起点以递归扩展依赖，触发w2⇔w3。

## 参考

https://zhuanlan.zhihu.com/p/619146631

https://developer.aliyun.com/article/1231617