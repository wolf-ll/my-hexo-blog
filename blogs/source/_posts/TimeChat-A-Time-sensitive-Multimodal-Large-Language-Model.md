---
title: 'TimeChat: A Time-sensitive Multimodal Large Language Model'
date: 2024-10-13 10:31:52
auther: 小狼
summary: 基于视频滑窗Q-Former的时序感知的视频大模型
categories: 论文
img: /medias/featureimages/10.jpg
tags:
  - MLLM
  - 视频理解
---

# TimeChat: A Time-sensitive Multimodal Large Language Model

## 摘要

本研究提出的 TimeChat 是一种时间敏感的多模态大型语言模型，专为长视频理解而设计。模型包含两个关键的架构贡献：（1）**时间戳感知帧编码器**，可以将视觉内容与每个帧的时间戳绑定；（2）**滑动视频 Q-Former**，生成不同长度的视频token序列，以适应不同时长的视频。此外，本文还构建了一个**指令微调（instruction-tuning）数据集，包含 6 个任务和总共 125K 个实例**，以进一步提高 TimeChat 的instruction-following性能。各种视频理解任务（如dense captioning, temporal grounding, and highlight detection）的实验结果都证明了 TimeChat 强大的**零样本时间定位和推理**能力。例如，与最先进的视频大语言模型相比，TimeChat 在 YouCook2 上获得了 +9.2 的 F1  score和 +2.8 的 CIDEr 分数，在 QVHighlights 上获得了 +5.8 的 HIT@1 分数，在 Charades-STA 上获得了 +27.5 的 R@1 分数（IoU=0.5），有望成为**长视频理解任务的多功能视频助手**，满足用户的实际需求。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241013105855336.png" alt="image-20241013105855336" style="zoom:80%;" />

## 引言

**背景**

* 从教育教程到故事电影，长格式的视频已经成为我们日常生活中的一个重要媒介。但筛选冗长的视频是相当耗时且无趣的工作。
* 人类的注意力总是被有意义的或突出的视觉片段所吸引，比如烹饪教程中的基本步骤或体育赛事中的精彩时刻。
* 一个智能的时间敏感视频助手，为用户分析长视频，包括**时间定位、时间戳检测和关键时刻总结**，是社会的长期需求。
* 大模型拥有强大的**指令遵循**能力，可以用于长视频理解任务，满足用户的现实需求。

**现状**

* 已经进行了一些初步探索以集成视觉编码器和LLM来实现基本的视频理解（字幕、问答）。
* 然而，现有的视频LLM（VidLLM）只能**捕获短片段的全局视觉语义**，而不能**将重要的视频内容与准确的时间戳关联**起来。
  * 例如，VideoLLaMA和VideoChat努力定位和描述未经修剪的视频中的有意义的事件，导致在Tab2中验证的准确性较低。
  * 个人理解：上述两种模型侧重识别特定事件发生，本文模型侧重时间戳定位。**在社区事件定位场景中需要取二者均衡。**

现有的VidLLM有两个问题：（1）刚性压缩将视频token转换为固定数字不适合长视频输入--它忽略了视频的持续时间，在处理长视频的大量帧时，导致严重的**时空语义退化。**（2）它们分别处理视频和时间戳信息，而不考虑显式的**时间-视觉关联**，因此无法准确地定位时间戳。

**本文贡献**

* 提出TimeChat，一种时间敏感的多模态大语言模型，用于长视频理解和准确的时间定位。
* 为了处理长视频输入，提出了一种**滑动视频Q-Former**来适应视频特征提取和压缩过程中的自适应视频标记长度。视频Q-Former将滑窗内的帧压缩为视频token。移动窗口可以动态创建不同长度的视频token序列。它保留了**长视频的重要视觉语义**，并得到了更具表现力和可伸缩的视频表示。（**捕获帧间时间信息**）
* 为了增强**视觉-时间戳的关联**，提出了一个具有时间感知能力的**帧编码器**，它显式地将视觉上下文与每个帧的时间戳描述绑定起来。（**绑定帧+时间戳信息**）

* 为了提高TimeChat固有的时间戳定位能力，增强它的指令遵循能力，构建了一个新的指**令调优数据集TimeIT**，涉及不同的**时间戳相关的用户指令**。该数据集是由各种与时间戳相关的长视频数据集编译而来的，平均视频长度为**190.8s**。它由6个不同的任务、12个广泛使用的学术基准和总共125K实例组成。（**增强时间-视觉关联能力**）

## 相关工作

许多研究都在努力将llm与视频编码器集成起来，从而利用llm的强大理解和生成能力来进行视频任务。这些研究通常使用开源的llm，如Vicuna和LLaMA。它们的关键区别在于它们**如何将视频编码为与llm兼容的视觉token**。有代表性的工作，如VideoChat利用一个video transformer来编码视频features，随后实现一个Query Transformer（Q-Former）来压缩video tokens。VideoLLaMA首先使用vision transformer（ViT）和图像Q-Former对独立帧进行编码，然后使用视频Q-Former进行时间建模。然而，这些方法将视频token压缩到一个固定的数字，导致在处理长视频时视觉语义退化。相比之下，本文模型为视觉标记提供了可调的压缩率，增加了对不同视频长度的适应性。此外，模型明确地建立了一个帧级的视觉-时间戳关系，以提高时间定位能力。

视觉-语言指令微调需要使用人工指令生成高质量的数据，这可以分为两个技术分支。（1）合并可用的多模态基准数据集，将它们转化为指令格式；（2）利用GPT生成更多样化的对话式数据。

时间定位是视频理解任务的一个基本能力，特别是对于未修剪的长视频。有各种时间敏感的视频任务，包括视频时间定位、密集视频字幕、视频摘要、视频亮点检测、步长定位等。这些任务需要在视频语义和相应的时间戳之间进行显式的关联。以前的研究倾向于在专门的下游数据集上单独处理每个任务。尽管最近的工作对弥合一些任务进行了初步的尝试，但基于llm的通用范式仍在探索中。本文在语言建模上统一了一些时间敏感的视频任务，迈出了充分利用llm的通用能力的第一步。

## 本文方法

模型组件主要由三部分构成：1）时间感知帧编码器，2）视频滑窗Q-Former，3）LLM

给定一个输入视频，帧编码器首先**独立地提取每个帧的视觉和时间戳特征**。接下来，视频Q-Former建模**滑动窗口内帧的时间关系**，以生成视频token。最后，将这些视频token与可选的转录语音和用户指令连接起来，然后将这些指令输入LLM以生成响应。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241013153746315.png" alt="image-20241013153746315" style="zoom:80%;" />

###  Timestamp-aware Frame Encoder

以前的研究通常将视觉语义的建模和输入帧各自的时间戳信息分开。该方法不能直接捕获视觉事件发生时的时间。一些方法为视觉标记了可学习的位置（时间）嵌入。然而，**这只能使模型能够识别帧的顺序，在确定精确的时间矩时缺乏精度。**

为了解决这些问题，引入受 InstructBLIP启发的时间戳感知帧编码器。给定一个视频输入V，首先使用一个预训练好的图像编码器即ViT对 每一帧进行编码获得帧特征（n\*n\*d），随后，一个图像Q-Former进一步压缩帧token。如图2所示，Q-Former以Dq维的可学习查询作为输入。这些查询通过交叉注意力与frame feature进行交互，并将初始查询更新到维度Dq中的Ni视觉标记。值得注意的是，在视觉标记抽取过程中，添加了帧的时间戳例如“This frame is sampled at 2s.”作为Q-Former融合视觉和时间戳信息的一个条件。

> 借鉴了InstructBLIP的Q-Former。首先通过ViT-G/14 from EVA-CLIP来抽取图片特征（n*n*d），再通过Q-Former来对feature做提取，并通过输入文本“This frame is sampled at 2s.”，来把时间戳信息也混合进去。一帧图像出来的特征是Ni*D的，Ni就是query向量的个数。这里的Q-Former是用InstructBLIP权重初始化的。

###  Sliding Video Q-Former

对于T帧的视频输入，使用时间戳感知帧编码器后，获取到T\*Ni\*D的视觉token。此时各帧独立编码，没有建模**帧间时间信息**。为此，引入Q-Former滑窗，在时间维度上增强特征融合。设计了一个长度为Lw的滑动窗口，并在每个窗口内利用视频Q-Former从Lw帧中提取Nv长度的视频token。（**滑窗Lw，步长S，Q-former查询向量数Nv**）最终可以将输入的视频表示为（T /S）×Nv的视频token。（上图黄色部分）

由于视频的三维特性，有大量冗余时空信息，原始视觉token会非常长（原始帧中的所有patch），需要压缩信息以降低LLM计算量。之前的工作一般都设置固定的视觉token数Nv比如32，当输入帧数T很大时，会造成严重的视觉语义退化。

本文采用固定的步长来保证长视频不会被过度压缩，即最终送入LLM的tokens数量会根据视频的长度变化而变化。在送入LLM之前还会经过一个线性映射层来使tokens的特征维度符合LLM的输入特征维度需求。

将压缩率R定义为**原始视觉标记的数量与最终视频标记的数量之比**。则以前Video-LLaMA的压缩率是：
$$
R=（T*N_P）/N_V
$$
其中，**Np为每一帧的patch数**。这个比率随着输入帧数T的增加而增加，并会导致长视频的过度压缩。使用滑动视频Q-Former，压缩率R'变成一个常数值：
$$
R'=\frac{T*N_P}{(T/S)*N_V}=\frac{S*N_P}{N_V}
$$
为长视频保留更丰富的语义。通过调整步幅S，可以根据计算预算来控制视频token的最终数量。最后，利用线性层对视频标记的维数DQ进行变换，以匹配LLM嵌入空间的维数DLLM。

### Large Language Model

将多种模态的输入连接起来，包括视频token Xv，文本查询向量Xq（包括可选的转录语音和用户指令），并将其输入一个大型语言模型，以生成合理和连贯的响应Xa。在这里，Xv、Xq和Xa具有相同的标记embedding维数DLLM。

VidLLM的训练通常采用**两阶段训练框架。**第一阶段使用大规模图像/视频-文本对进行预训练，以实现**视觉-语言对齐**。第二阶段使用指令数据对模型进行微调，以实现**指令遵循**。考虑到计算效率，本文重用了在第一阶段训练后现有开源模型的检查点（模型用的是LLaMA-2 (7B)），**仅进行指令微调**。采用LoRA微调的方式来对LLM模型进行微调。在训练过程中，利用**语言建模损失**生成长度为LT的目标答案Xa：

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241014144753535.png" alt="image-20241014144753535" style="zoom:80%;" />

### 数据集TimeIT

为了提高TimeChat对时间敏感的人类指令的理解能力，提出TimeIT，一个涉及时间戳的以视频为中心的指令调优数据集。该数据集集成了广泛的与时间戳相关的视频数据集，并以长篇视频为特征。

TimeIT包含了6个与时间戳相关的视频任务，即：(1) 密集视频字幕生成，(2) 视频时间定位，(3) 步骤定位和文字生成，(4) 视频摘要，(5) 视频亮点检测，以及 (6) 转录语音生成。它还整合了来自不同领域的12个特定数据集。数据集适应了在现实世界应用中与AI助手交互时涉及视频时间戳的普遍用户请求。

数据集构建方式分两步：1）Instruction Writing  和  2) Answer Formatting。

**指令构造**：先通过人工进行构造，然后利用GPT-4进行扩写来产生更多样化的表达，最后通过人工检查和refine来形成最终的版本。对于**每个task会产生6个高质量的指令**（instructions）。

**答案模板**：根据编写的指令，我们进一步将任务输出重新表述为用户友好的自然语言响应范式。考虑到所涉及的视频数据集是人工收集的，TimeIT数据的整体质量得到了保证。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241014164810146.png" alt="image-20241014164810146" style="zoom:80%;" />

表1将TimeIT数据与现有的视频指令调优数据进行了比较，揭示了本文数据集在数据规模、任务多样性和视频长度方面的显著优势。附录C提供了每个任务对模型性能的贡献分析。总的来说，所有6个任务都是有益的。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241014165334936.png" alt="image-20241014165334936" style="zoom:80%;" />

## 实验

以EVA-CLIP中的ViT-G/14作为图像编码器，以LLaMA-2（7B）作为语言基础模型。图像Q-Former的参数从InstructBLIP的检查点初始化，而视频Q-Former从Video-LLaMA的检查点初始化。在TimeIT和Valley上调整了3个epoch，使用32的批处理大小，使用一台8-V100（32G）机器。如图2所示，ViT和LLM的参数被冻结，而图像Q-Former、视频Q-Former和线性层的参数被调整。窗口大小Lw、步幅S和每个窗口的视频token Nv数为32。输入帧数为96。其他超参数请参阅附录D。

* 模型在三个任务上进行零样本长视频理解评估，即字幕（说明性文字）生成，时间定位，和亮点检测。评估数据集包括YouCook2、 Charades-STA和 QVHighlights。评价指标的细节详见附录E。

* 用于解析LLM输出的启发式规则：由llm生成的输出可能包括口语化表达式，从而导致较大的响应变化。因此，作者设计了大量的启发式规则，以保证能够准确地从模型的响应中提取预测的答案，以计算最终的度量。
* 方法比较：将模型与两个基线对比。（1）**多模态pipeline**： VideoChat-Text，InstructBLIP+ChatGPT。这些pipeline将专用的视觉模型与GPT集成，首先将视觉语义转换为文本描述，然后利用ChatGPT来处理所有输入来解决目标任务。（2）**端到端模型**：Valley，VideoChat-Embed，Video-LLaMA with 7B LLMs。这些模型直接将视频作为输入，并以端到端的方式生成响应。

### 零样本性能

表2显示了TimeChat(7B)的零样本性能，它在所有任务中都优于以前的VidLLM(7B/13B)。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241014173705687.png" alt="image-20241014173705687" style="zoom:80%;" />

**密集字幕/说明文字生成**：该任务在YouCook2上训练。模型需要在平均320秒的视频持续时间内，准确地识别出大约8个基本的烹饪步骤，并提供与视觉内容相匹配的忠实描述。烹饪的特殊性也提高了任务的复杂性，挑战了模型的通用性。现有的端到端VidLLM难以实现精确的moment定位，性能最好的VideoChat-Embed模型获得的低F1分数3.4就证明了这一点。**这种moment定位的不精确显著影响了说明文字的评估，使得SODA_c和CIDEr指标都接近于零。**与之相比，本文模型通过+1.0 SODA_c、+2.8 CIDEr和+9.2 F1评分获得了显著的SOTA。这表明TimeChat能够有效地处理长时间的视频，拥有精确的时间定位能力。此外，本文模型性能也显著超过了由ChatGPT提供的多模态pipeline（F1评分从8.4到12.6）。

**视频亮点检测**：dense video captioning任务集中于在clip级别定位事件，而亮点检测任务需要在帧级别进行更细粒度的视频理解。对于输入视频，其目标是输出亮点帧的时间和突出分数。整体来看，本文模型在QVHhemict上达到了14.5 mAP和23.9 HIT@1，比之前的vidllm分别获得了+1.4和+5.8分。这突出了**时间戳感知帧编码器在识别每个帧的显著语义方面**的贡献。此外，该任务是TimeIT的held-out任务，表明了模型对新任务的泛化能力。**多模态pipeline方法获得了更好的性能**，作者推测，这是由于高亮检测的格式与他们的方法更兼容，鉴于模型接收到一系列的输入帧的联合的时间戳-视觉描述。这使得LLM能够逐帧进行评估，从而促进更准确的判断。

**时间定位**：此任务旨在识别查询语句所描述的对应时间戳。TimeChat在Charades-STA数据集的“R@1，IoU=0.5”上达到32.2分，大大超过了之前的SOTA端到端VidLLM，即Valley（+27.5）。这表明，本文模型能够准确地定位给定文本查询的视频时刻。值得注意的是，TimeChat在时间定位任务上取得了最大的进步，作者认为该任务主要强调了**长视频的时间定位能力**，而这正是TimeChat的最佳优势。

### 定性评估

图4显示了在零样本设置下，TimeChat和其他VidLLM之间的定性比较。Video-LLaMA没有完全理解用户的指令，因为它只描述了步骤，而没有给出相应时间戳。VideoChat生成了符合请求格式的说明，但错位了所有步骤的时间。此外，VideoChat生成的描述包含一些幻觉。与之相比，TimeChat显示了改进的时间定位和总结功能。它成功地匹配了几乎所有提取的剪辑的视频内容。此外，幻觉的发生也明显减少了。然而，在增强模型生成的摘要的丰富性和细节方面仍有改进的空间。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241015171454792.png" alt="image-20241015171454792" style="zoom:80%;" />

**领域推广**：在附录G中，展示了新领域的定性结果，如movie和egocentric videos，展示了TimeChat对新场景的泛化。这种泛化是一个实用的视频助手的一个关键特征，它代表了基于LLM的TimeChat和当前为特定的下游数据集量身定制的专用模型之间的根本区别。

### 消融实验

当**删除滑动视频Q-Former**时，最终视觉token的数量从96减少到32，导致信息压缩率为3倍。语义信息的减少导致了生成的描述和视频内容之间的对齐性的降低。具体来说，SODA_c度量减少了1.0，而CIDEr度量减少了2.8。此外，时间戳的准确性（以F1分数衡量）降低了3.0。在**去除时间戳感知帧编码器**的情况下，模型对时间基础描述的能力显著下降，F1分数下降了2.3。这些结果突出了模型中两个新模块的有效性。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241015173335537.png" alt="image-20241015173335537" style="zoom:80%;" />

### 其他分析

为了证明模型的性能提高不仅归因于新的TimeIT数据集，而且还归因于模型架构的改进，**只使用YouCook2数据集进行微调和评估**。在这个设置中，使用现有的开源检查点初始化模型（参见4.1）。对于所有的模型，应用LoRA 并微调Q-Former。Tab.4给出了结果，显示模型在所有指标上都始终优于以前的模型。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241015174655078.png" alt="image-20241015174655078" style="zoom:80%;" />

在图5中，作者检查了模型关于输入帧数的性能测量。如3.1.3中提到的，以前的模型如Video-LLaMA和VideoChat压缩了长视频的过多信息，当输入帧数从32增加到96时，性能表现最差。相比之下，本文模型使用滑动视频Q-Former解耦帧数T和压缩率R‘。随着帧数的增加，曲线表现出线性提高，显示出优越的可伸缩性。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241015175208335.png" alt="image-20241015175208335" style="zoom:80%;" />



### 与专用模型的比较

比较TimeChat与在三个任务上分别拥有最先进性能的专用模型。鉴于所有专用模型都在特定数据集上做了微调，作者对自己的模型也做了微调以进行公平比较。如表5所示，微调后TimeChat进一步提升了性能（+6.9 F1 score on YouCook2, +16.9 HIT@1 on

QVHighlights, and +16.4 R@1 (IoU=0.5) on Charades-STA）。专用模型的优越性来自特定任务的设计，例如Vid2Seq在YT-Temporal-1B上做预训练，这个数据集具有更多高质量的长视频；QD-DETR采用一种特殊的显著性token进行显著性预测，并引入了4个损失函数用于训练，而本文模型纯粹通过语言建模进行训练。此外，这些模型还使用了更多的微调步骤，以更好地适应下游数据集。

而作为一个通用模型，TimeChat在零样本场景、多任务和多领域设置中表现出很强的泛化能力，而这在这些专家模型中是不存在的。在每项任务上实现最先进的性能并不是本文的主要目标。

<img src="TimeChat-A-Time-sensitive-Multimodal-Large-Language-Model\image-20241015190311946.png" alt="image-20241015190311946" style="zoom:80%;" />

## 总结

本文提出了TimeChat，一个用于长视频理解的时间敏感的VidLLM。得益于新的时间感知帧编码器、滑动视频Q-Former和TimeIT上的指令调优，本文模型显示了强大的时间定位能力，这是在以前的VidLLM中所没有的。通过在冗长的视频中识别重大事件，确定事件的开始和结束时间，并生成简明的总结，TimeChat向智能视频助手迈出了关键的一步。在未来，将进一步取得架构上的进步，以提高视频语义密度，同时减少时空冗余。作者还将收集更多样化和高质量的指令调优数据，以扩大与时间相关的应用。

## 参考

[TimeChat：基于Q-Former的时序感知VideoLLM - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/672514787)

