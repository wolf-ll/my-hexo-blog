---
title: 'ShareGPT4Video: Improving Video Understanding and Generation with Better Captions'
date: 2024-12-03 15:53:21
auther: 小狼
summary: 多模态大模型视频理解能力基准
categories: 论文
img: /medias/featureimages/8.jpg
tags:
  - MLLM
  - 视频理解
  - 字幕生成
---

# ShareGPT4Video

## 摘要

论文推出了ShareGPT4Video系列，旨在通过**密集且精确的标注**促进大型视频语言模型（large video-language models, LVLMs）的**视频理解**和文本到视频模型（text-to-video models, T2VMs）的**视频生成**。该系列包括：

1）**ShareGPT4Video**，包含4万个GPT4V标注的密集视频字幕，这些视频具有不同长度和来源，通过精心设计的数据过滤和标注策略开发。

2）**ShareCaptioner-Video**，一个高效且能力强大的**任意视频标注模型**，由其标注了480万个高质量美学视频。

3）**ShareGPT4Video-8B**，一个简单却卓越的LVLM，在三个前沿视频基准测试中达到了最先进（SOTA）性能。

为了达成上述目标，不考虑不可扩展且成本高昂的人工标注，作者发现使用GPT4V对视频进行标注，采用简单的多帧或帧连接输入策略，会导致**细节不足，生成的字幕不够详细且会出现时间混淆**的结果。针对这一问题，作者认为设计高质量视频标注策略的挑战在于三个方面：**1) 帧间精确的时间变化识别。2) 帧内详细内容描述。3) 任意长度视频的帧数可扩展性。**

为此，论文精心设计了一种**差分视频标注策略**，该策略稳定、可扩展且高效，适用于生成**任意分辨率、宽高比和长度的视频标注**。基于此，论文构建了ShareGPT4Video，包含4万个高质量视频，涵盖广泛类别，其标注涵盖丰富的**世界知识、物体属性、摄像机运动**，以及关键的、详细且精确的**事件时间描述**。基于ShareGPT4Video，论文进一步开发了ShareCaptioner-Video，一个**能够高效生成任意视频高质量标注的优秀标注器。**本文通过它标注了480万个具有美学吸引力的视频，并在**10秒文本到视频生成任务**中验证了其有效性。对于视频理解，论文验证了ShareGPT4Video在几种当前LVLM架构上的有效性，并展示了论文卓越的新大型视频语言模型 ShareGPT4Video-8B。

<img src="ShareGPT4Video-Improving-Video-Understanding-and-Generation-with-Better-Captions\image-20241203171739454.png" alt="image-20241203171739454" style="zoom:67%;" />

## 引言

LLM的驱动下，多模态学习中**图文对话**（image-text dialogue）和**文生图**任务（text-to-image generation tasks）已经取得了最新进展。这也激发了向**视频理解**和**生成任务**的转变，允许用户跨视频和语言模式的交互。因此，详细和高保真的视频字幕，连接上述模态，有助于推动该领域的进展。

尽管视频内容具有丰富的语义和时间信息，但现有数据中视频通常**只配有简短的描述**。这些简短的描述**限制了对视频的深入理解以及视频生成的可控性**。虽然图像-文本对话和文本到图像生成任务中已认识到详细描述的重要性，但在视频理解和生成方面类似的努力仍然不足。

然而，创建大规模、高质量的视频描述是一项挑战性任务。即使是人类，为长视频生成详细的描述也是**复杂且耗时**的，这阻碍了大规模的标注工作。**当前的开源大规模视觉语言模型（LVLMs）缺乏这种能力，而闭源API尚不支持视频输入。**

另一方面，如果将视频输入**简化为多个帧**，即使是GPT4V也难以提供满意质量的描述。例如，一个直观的想法是向GPT4V提供带有时间戳的多帧视频，并生成描述，但论文发现GPT4V在描述时表现不稳定，有时会**误解帧之间的时间关系**，且**随着视频帧数的增加，其性能进一步下降**。其他解决方案，如将所有帧拼接成一张大图，**对于解决时间问题并无帮助**，且随着帧数的增加，描述的细节会丢失。论文在图11-12中展示了这些问题。

论文认为，制定有效的视频描述策略的挑战源于三个基本方面：1) **帧间精确的时间变化理解**：时间维度将视频与图像区分开来。不精确的时间描述会显著降低视频描述的质量，并在训练模型时引起混淆。2) **帧内详细内容的描述**：详细描述对于图像与文本模态之间的对齐至关重要，对于视频-文本对齐也同样重要。3) **任意长度视频的帧数可扩展性**：在实际应用中，视频的长度差异很大。理想的描述策略应能适应这种变化，并为任何长度的视频生成适当的描述。

为此，论文提出了**差分滑动窗口**标注策略 **Differential Sliding-Window Captioning strategy** (DiffSW)，该策略稳定、可扩展且高效，适用于为任意视频生成描述。DiffSW的核心理念是**将  所有帧到描述（all-frames-to-caption）  的任务转化为差分描述任务**。具体而言，本文**为第一帧生成详细字幕**，然后按时间顺序对后续帧应用长度为二的滑动窗口。GPT4V负责基于三个输入（前一帧、其差分字幕和当前帧）识别帧间变化。这包括了照相机运动、对象运动、角色动作和场景转换。获取所有差分字幕后，这些字幕将输入到GPT4生成**整个视频的综合描述**。差分概念使DiffSW能够**集中于帧间的变化**，即**时序变化**。滑动设计确保**了时间顺序的正确性和帧数的不变性**，保证GPT4V不遗漏细节并高效利用API，从而实现稳定、可扩展和高效的字幕质量。此外，差分设计允许通过重用其差分字幕来重新标注带字幕视频的任何子剪辑。

基于DiffSW，本文构建了**ShareGPT4Video**，包含4万组高质量的视频-字幕对，涵盖广泛类别，由此产生的字幕包含了丰富的世界知识、对象属性、相机运动，以及对事件的详细和精确的时间描述。ShareGPT4Video的视频来自不同来源，采用**基于语义的数据过滤**策略来减轻这些视频之间的内容同质性。然后对视频采用**语义感知的关键帧提取**策略，以减少时间冗余。将DiffSW应用于关键帧，生成高质量的字幕，并通过**层次提示设计**进一步提高了其稳定性和质量。采用**人工质量检查**，以确保视频字幕的质量。

基于ShareGPT4Video，本文提供了**ShareCaptionor-Video**,，一种特殊的**视频标注器**，能够有效地为广泛的**分辨率、纵横比和持续时间**的视频**生成高质量的字幕**。它能够以较低的成本和令人满意的质量，进一步**扩展高质量的视频字幕数据**，本文作者为480万个视频（总计约3000小时）生成高质量的字幕。

作者在视频理解和生成任务中进行了广泛的实验，以证明高质量的视频字幕数据集和优越的视频标注器的价值。在视频生成方面，在4.8M视频-字幕对上训练的基于DiT的文本到视频模型在生成10秒高分辨率视频-字幕对内容生成的细粒度控制方面表现良好。为了理解视频，ShareGPT4Video通过替换一小部分训练数据，在多个基准测试上为多个当前lvlm带来了一致的性能提高。本文进一步提出了ShareGPT4Video-8B，一个简单而出色的LVLM，在三个先进而全面的视频基准上达到了SOTA的性能。

## ShareGPT4Video数据集



## 参考

[[2406.04325\] ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://arxiv.org/abs/2406.04325)

[ShareGPT4Omni/ShareGPT4Video: [NeurIPS 2024\] An official implementation of ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://github.com/ShareGPT4Omni/ShareGPT4Video)

[【LLM】ShareGPT4Video：借助更优质的标题提升视频理解和生成能力 - 知乎](https://zhuanlan.zhihu.com/p/702413750)

