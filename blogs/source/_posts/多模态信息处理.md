---
title: 多模态信息处理
date: 2024-09-09 14:22:20
auther: 小狼
summary: 多模态概念，理论，应用，模型等整理，自用
categories: 笔记
img: /medias/featureimages/14.jpg
tags:
  - LLM
  - 多模态
---

# 多模态信息处理

# 综述（2022）

《多模态信息处理前沿综述：应用、融合和预训练》 2022

## 引言

* 人工智能研究经过70多年的探索，在视觉、语音与声学、语言理解与生成等单模态人工智能领域已取得了巨大的突破。
* 近些年，如何让计算机拥有更接近人类的理解和处理多模态信息的能力，进而实现高鲁棒性的推理决策成为热点问题。
* 各种应用的涌现，对多模态信息处理技术在用户理解、内容理解和场景理解上提出了更高的要求，同时也给多模态技术提供了海量的数据和丰富的应用场景。
* 多模态核心技术又分为：**多模态表示（Representation）、多模态融合（Fusion）、多模态转换（Translation）、多模态对齐（Alignment）和模态协同学习（Co-learning）**
* 本文从自然语言处理的视角出发，介绍多模态信息处理技术的最新进展，组织结构如下：第１节介绍NLP领域关注度较高的多模态应用和相关的数据集。多模态融合是多模态信息处理的核心问题。第２节从单模态信息的表示方法、多模态信息的融合阶段、融合模型的网络结构、未对齐模态和模态缺失情况下的多模态融合等角度介绍主流的多模态融合方法。第３节介绍多模态预训练技术，并从模型的网络结构、模型的输入、预训练目标、预训练语料和下游任务等维度对比最新提出的多模态预训练模型。第４节介绍多模态技术在工业界的应用。最后一节是总结和对未来工作的展望。

## 多模态应用

<img src="多模态信息处理\image-20240909145625058.png" alt="image-20240909145625058" style="zoom: 33%;" />

### 多模态情感识别

* 在交互场景下，多模态情感识别研究如何从**人的表情和动作手势、语音音调、语言**等多模态信息中理解用户细颗粒度的情感表达，进而指导人机交互策略。
* 其主要研究内容有：**①基于多模态信息互补性和异步性的动态融合；**②高噪声环境下对于**模态模糊或模态缺失问题**的鲁棒性融合；③客服和营销等**自然交互情境**下的情感识别等。

<img src="多模态信息处理\image-20240909151102370.png" alt="image-20240909151102370" style="zoom:67%;" />

### 视觉-语言生成

* 视觉（图像或视频）到语言的生成和语言到视觉（图像或视频）的生成打破了计算机视觉和自然语言处理两个领域的边界
* 2021年初，OpenAI推出的基于GPT-3的语言到视觉的生成模型DALL-E可以根据自然语言的描述生成逼真的图像

#### 图像描述

* 图像描述（Image Captioning）是**对给定的一幅自然图像生成一句自然语言描述的任务。**
* 2015以前：基于模板--监测图像物体、动作，填充到模板主谓宾结构
* 2015以后：通过从视觉图像中解析出属性、关系和结构（hierarchy）等高层语义信息，并将这些语义信息融入**视觉编码和语言解码**中，提高了图像描述的生成效果。

#### **视频描述

* 视频描述（Video Captioning）是对给定的一段视频（通常是几十秒的短视频）生成一句准确、细致描述的任务。
* 包含**图像、声音、时序**等信息。视频描述可提取的特征更多，技术挑战也更大。
* ACTIONS是首个无需人工标注、从数以亿计的网页内容中自动提炼“视频，描述”对的视频描述数据集，总共包含了163183个GIF视频。

#### 视觉叙事

* 视觉叙事（Visual Story Telling）要求模型对于给定的图像序列，在深度理解图像序列的基础上**生成连贯的叙事故事**。
* 视觉叙事的输入是**有时序关联的图像序列**，需要模型具备**根据历史视觉事件推测当前的视觉事件**的能力。
* 对比图像描述和视频描述中的客观文字描述，视觉叙事的输出由更多评价性、会话性和抽象性语言组成。

### 视觉问答和多模态对话

#### 视觉问答（VQA）

* 给定一幅图像和一个关于该图像的开放式自然语言问题，要求模型准确回答该问题
* 视觉问答是一个典型的多模态问题，需要模型具备**物体定位、属性检测、事件分类、场景理解和推理及数学计算**等能力。
* 根据图片类型不同，VQA又分为自然图像理解VQA、合成图像推理VQA和自然图像推理VQA

<img src="多模态信息处理\image-20240909151819273.png" alt="image-20240909151819273" style="zoom:67%;" />

#### **视觉对话

* 视觉对话（Visual DIalog）是给定一幅图像（或视频等视觉内容）和一个上下文相关的问题，要求模型根据图片（或视频）内容回答该问题。
* 与视觉问答相比，视觉对话还要解决对话中特有的挑战，如共指（Co-references）和省略（Ellipsis）等。
* 视觉对话中的用户问题只与单个图像（视频）相关，且用户问题和模型回答都是文字的。

#### 多模态对话

* 多模态对话（Multi-model Dialog）关注更接近人类自然对话的多模态人机对话技术研究。它与上一节介绍视觉对话的主要差异有：
  * ①多模态对话给定的输入图像可能是多幅的；
  * ② 随着对话的推进，**图像是不断更新的；**
  * ③用户问题和模型的回答可以是**文本的、图像的或者图文结合的；**
  * ④模型可能需要查询外部领域知识库才能回答用户的问题（如购物者希望看到更多与特定商品相似的商品，或者要求提供满足某些特征的商品，或者查询特定商品的属性等）；
  * ⑤模型可能需要通过反问等对话策略澄清用户需求。

<img src="多模态信息处理\image-20240909151958971.png" alt="image-20240909151958971" style="zoom:67%;" />

### 多模态摘要

* 多模态摘要是基于对多模态输入（文本、语音、图像和视频等）的理解，归纳并生成单模态或者多模态的概括性总结（摘要）任务。

* 根据具体任务类型，多模态摘要又可细分为**视频会议摘要、教学视频摘要、多模态新闻摘要 和多模态商品摘要**

### **多模态对齐

* 多模态对齐研究多个模态不同颗粒度元素间的对齐关系，具体又分为显式对齐和隐式对齐。
* 视觉－语言跨模态的显式对齐任务研究**图像和句子、图像和词、图像中的目标和句子中的短语间的**对齐关系。
* 多模态对齐方法可直接应用于**多模态检索**等应用，也可作为**图像描述、VQA、多模态预训练**的训练语料，尤其是在缺乏大规模多模态人工标注语料的场景。
* Tan等人提出了 Vokenization技术（图３），其通过给文本中的每一个词打上一幅图像的标签，实现在大规模文本语料上自动构建多模态对齐语料库。

<img src="多模态信息处理\image-20240909152259522.png" alt="image-20240909152259522" style="zoom: 33%;" />

#### 图像短语定位

* 图像中的目标和文本中的短语对齐也被称为**图像短语定位**（Phrase Grounding），可用于提高图像描述、VQA、视觉导航等视觉-语言下游任务的性能。
* Plummer等人发布了一个大规模的短语定位数据集Flickr30kEntities，如图4所示。

<img src="多模态信息处理\image-20240909152343325.png" alt="image-20240909152343325" style="zoom:67%;" />

#### **视频定位

* 视频定位（Video Grounding）是多模态对齐中另一项重要且具有挑战性的任务。给定一个查询（Query），它要求模型从视频中定位出与查询语言对应的一个目标视频片段。该技术可应用**于视频理解、视频检索和人机交互**等场景。

### 多模态翻译

* 多模态翻译是将多模态输入（文本、图像或视频等）中的源语言文本转换为目标语言文本的过程。
* Elliott等人将多模态机器翻译分解为两个子任务：**文本翻译 和 基于视觉的文本表示**
* Wu等人的研究表明，视觉特征对多模态翻译的帮助来自于正则化，视觉特征的合理选取对模型性能至关重要。

### 多模态信息抽取

* 命名实体识别（NER）是指识别自由文本中的具体特定意义的实体（如人名、地名和组织机构名等）。
* 多模态命名实体识别（ＭＮER）通过引入视觉、语音等其他模态作为文本模态的补充，识别社交媒体中高噪声短文本中的实体。
* 模型方面，Moon等人首次提出了融合图像和文本模态信息的通用多模态注意力模型。
* Yu等人首次将 Transformer应用于多模态NER任务中，并提出了实体片段检测辅助任务，进一步消除视觉偏差，提升了模型效果

## 多模态融合

* 多模态融合将多个单模态表征整合成为一个多模态信息表征，它是多模态信息处理的核心问题。
* 多模态融合的研究方向有：基于多模态互补性的全模态融合问题、模态模糊或者模态缺失下的鲁棒性融合问题、非对齐的多模态融合问题等。

<img src="多模态信息处理\image-20240909154906558.png" alt="image-20240909154906558" style="zoom: 80%;" />

### 根据单模态表示分类

* 单模态的特征表示是多模态融合的基石。这一类方法重点研究如何在多模态融合之前提取更好的单模态特征表示。
* 以视觉－语言－音频多模态应用为例，如何从**视觉内容中解析出高层语义信息**以增强视觉特征表达是这一类方法的主要研究内容。
* 语言表示通常使用词的独热编码表示、词的上下文表示 、句子表示、句法依存关系表示、场景图表示等。
* 音频表示可使用基于VOVAREP提取底层声学特征表示、基于预训练模型wav2vec提取低维特征向量表示等。

#### 视觉全局表示

* 视觉全局表示（Global Representation）是从**图像编码器的高层网络提取一个D维静态向量v**表示一幅图像。
* 相关工作通常**使用预训练的ResNet对图像编码**，再提取ResNet的最后一个池化层作为视觉全局表示（ResNet152池化层输出是1*2048维向量，即D=2048）。
* 视觉全局表示可用来**初始化多模态自动摘要模型的解码器**，或作为一个**特殊的字符与文本字符拼接**，再用递归神经网络对拼接的字符序列编码，或通过注意力机制学习与其他模态特征的**联合表示**等。
* 由于视觉全局表示将图像信息压缩到一个静态的向量中，**这可能会导致大量图像细节信息的丢失。**

#### 视觉区域表示

* 视觉区域表示（Regional Representation）是从图像编码器的高层网络中提取一组 Ｄ 维向量表示一幅图像。每个 Ｄ 维向量表示图像中特定的大小相同的区域
* 视觉区域表示与注意力机制相结合，通过在每一步解码过程中关注不同的图像区域可生成内容丰富的图像描述
* 视觉区域表示实现了图像的细颗粒度表示，但是每个特征的感受野大小和形状相同，**同一个目标可能被切分到多个区域中，它无法表达视觉上完整的语义信息。**

#### 视觉目标表示

* 视觉目标表示也是用一组Ｄ 维向量表示一幅图像，但每个 Ｄ 维向量表示图像中的一个目标。
* 预训练Faster Ｒ-CNN 通常被用来检测目标所在的区域，再使用目标所在区域的视觉特征和边界框（Bounding-box）特征作为该视觉目标表示。
* 视觉目标表示通过目标定位与分类实现视觉图像的浅层语义理解，但**它无法刻画图像中多个目标间的语义关系**

#### 视觉场景图表示

* 视觉场景图表示（scene graph representation）是用场景图Ｇ=（V，R）表示一幅图像。
* 场景图中的节点V={v1, v2...vk}是图像中的目标集合，关系R={r1, r2...rk}是图像中目标和目标间的显式语义关系（如 wearing、eating）、空间位置关系（如 cover、intersect、in）和隐式语义关系的集合。

<img src="多模态信息处理\image-20240909155741554.png" alt="image-20240909155741554" style="zoom:67%;" />

### 根据融合阶段进行分类

* 早期融合的特点是单模态表示学习简单，而多模态融合部分的模型深度大，融合策略复杂。例如，词的独热编码表示和视觉区域表示直接参与多模态融合
* 晚期融合的特点是单模态表示学习模型复杂，多模态融合一般采用拼接、按位乘／求平均等简单策略
* 在第３节介绍的多模态预训练模型中，基于单流架构（single stream）的预训练模型把融合操作放在早期阶段。
* 基于双流架构（two stream）的预训练模型则把融合操作放置在深层模型的中期阶段的多个层中

### 根据融合方式进行分类

#### 门控融合

* 基于自编码和自回归的大规模预训练语言模型和在下游任务上的微调相结合是自然语言处理研究和应用的新方法
* Rahman提出了一种多模态适应门（Multi-model Adaptation Gate，MAG）的网络结构将非语言特征（视觉和声学特征）与文本预训练语言模型融合，MAG与BERT结 合 （MAG-BERT）以 及MAG与XLNET结合（ＭＡＧ－ＸＬＮＥＴ）都可以有效融合三 个 模 态 信 息，并在多模态情感识别数据 集CMU-MOSI和CMU-MOSEI上获得当时最优性能。

#### 注意力融合

* Bahdanau等人在2015年提出的注意力机制是为了让神经机器翻译模型中的**解码器**在每一步解码过程中，有针对性地选择源语言中**“对齐”的词**来指导目标语言的解码，包括全局注意力和局部注意力两种方法。
* Yang等人提出了stacked attention networks（SANs），通过多层视觉注意力机制逐步过滤掉图像中的噪声区域，定位到与答案高度相关的图像区域，从而提高 ＶＱＡ 准确率。
* Anderson等人提出一种“自底向上”和“自顶向下”相结合的注意力机制。具体的，基于Faster R-CNN的“自底向上”的注意力机制提取图像中的兴趣区域，“自顶向下”的注意力机制确定兴趣区域的权重。
* Yu 等人提出了一种类 Transformer 结构的协同注意力机制，可实现文本中的任一词与图像中的任一区域间的完全交互。

#### Transformer融合

* BERT凭借着 transformer强大的特征学习能力和掩码语言模型（Masked Language Model）实现双向编码，刷新了多个 NLP任务的最优性能。
* 基 于transformer的多模态融合又分为单流模型和双流模模型两大类。 单流模型使用一个transformer在一开始便对多模态信息进行充分的交互。双流模型则对不同的模态使用独立的transformer编码，再通过协同注意力机制实现不同模态间的融合，如图７所示。

<img src="多模态信息处理\image-20240909155859453.png" alt="image-20240909155859453" style="zoom: 80%;" />

#### 图模型融合

图模型融合方法将不同模态数据建模为图结构，并利用图神经网络（Graph Neural Networks）等方法进行图结构的表示和学习，从而实现对多模态信息的融合和推理。Yin 等人将基于图的多模态融合编码器应用到多模态神经机器翻译模型中，把源语言中的词和图像中的物体放到了同一个图中，再堆叠多个基于图神经网络的多模态融合层（在每一层顺序执行模态内融合和模态间融合）。

## 多模态预训练

* 通过预训练语言模型从海量无标注数据中学习通用知识，再在下游任务上用少量的标注数据进行微调，已经成为自然语言处理领域成熟的新范式。
* 相对于文本预训练语言模型，多模态预训练模型可以更好地对细颗粒度的多模态语义单元（词或者目标）间的相关性进行建模。

<img src="多模态信息处理\image-20240909160214538.png" alt="image-20240909160214538" style="zoom: 67%;" />

* 从表５中的１１个图像－语言跨模态预训练模型的对比，我们发现的跨模态预训练模型的特点如下

  * ①单流模型和双流模型均被广泛采用。

  * ②多模态预训练模型从应用于多模态理解任务或多模态生成任务发展到可兼顾多模态理解和生成两大任务的统一模型。

  * ③相对动辄上百 Ｇ 甚至 Ｔ 级别的单模态数据，多模态对齐数据的规模有限。

    最新的多模态预训练模型可以利用互联网上的大规模非对齐的文本数据、图像数据、以及文本－图像对齐数据学习更通用的文本和视觉表示，以提高模型在视觉和语言的理解和生成能力，如 M3P和UNIMO。

  * ④多模态预训练模型从仅应用于多模态下游任务发展到可同时应用于单模态下游任务 和 多模态下游任务。

# 多模态大模型

多模态大语言模型（MLLM）是近年来以强大的大语言模型（LLM）作为大脑任务的多模态研究热点。MLLM令人惊讶的涌现能力，比如基于图像写故事和无ocr的数学推理，在传统方法中是罕见的，这表明了一条通往人工通用智能的潜在道路。本文旨在对MLLM的最新研究进展进行跟踪和总结。首先，我们提出了MLLM的公式，并描述了它的相关概念。然后，我们讨论了关键的技术和应用，包括多模态指令调整（M-IT）、多模态上下文学习（M-ICL）、多模态思维链（M-CoT）和llm辅助视觉推理（LAVR）。最后，我们讨论了现有的挑战，并指出了很有前景的研究方向。鉴于MLLM的时代才刚刚开始，我们将继续更新这项调查，并希望它能激发更多的研究。

## 引言

LMM在上下文学习In-Context Learning (ICL)，指令遵循instruction following，以及思维链Chain of Thought (CoT) 等方面都表现良好。但是仅限于理解离散文本，对视觉不敏感。 作为对比，大视觉模型Large Vision Models (LVMs）可以捕获视觉特征，但是推理落后。

鉴于此，多模态大模型Multimodal Large Language Model (MLLM)应运而生。在形式上，它指的是基于llm的具有接收、推理和输出多模态信息的能力的模型。在MLLM之前，有很多关于多模态的研究，可以分为判别的，生成的。CLIP，作为前者的代表，将视觉和文本信息投射到一个统一的表示空间中，为下游的多模态任务建立了一个桥梁。相比之下，OFA 是后者的代表，它以序列到序列的方式统一了多模态任务。

MLLM与传统模型相比，有两个具有代表性的特征： (1) MLLM是基于具有十亿尺度参数的LLM，这在以往的模型中是没有的。(2) MLLM使用新的训练范式来充分释放其全部潜力，例如使用多模态指令调优来鼓励模型遵循新的指令。有了这两个特性，MLLM展示了新的功能，比如基于图像编写网站代码，理解模因的深层含义，以及无ocr的数学推理。

## 架构

MLLM通常包含以下三个组件：

- A pre-trained modality encoder  一个预训练的模态编码器（接收和处理视觉/听觉等信号）
- A pre-trained LLM 一个预训练的大模型（理解和推理信号）
- A modality interface to connect them 一个连接他们的模态接口（对齐不同的模态）
- 一些mllm还包括一个生成器，用于输出除文本之外的其他模式。

<img src="多模态信息处理\image-20240912200539075.png" alt="image-20240912200539075" style="zoom:80%;" />

### 模态编码器

编码器将原始信息，如图像或音频，压缩成一个更紧凑的表示形式。一种常见的方法是，使用一个**预先训练过的编码器**，而不是从头开始进行训练。例如，CLIP通过对图像-文本对的大规模预训练，集成了一个在语义上与文本对齐的视觉编码器。通过对齐预训练，使用这种最初预对齐的编码器与llm对齐更容易。

表1总结了一系列常用的图像编码器的使用情况。

<img src="多模态信息处理\image-20240912203228022.png" alt="image-20240912203228022" style="zoom:80%;" />

在选择编码器时，通常会考虑**分辨率、参数大小和预训练语料库**等因素。许多工作已经验证，使用更高的分辨率可以实现显著的性能提高。放大输入分辨率的方法可以分为直接缩放和patch分割方法。

不同架构对于处理不同分辨率的图片有着不同的策略。

- **使用不同编码器编码后进行特征融合**：CogAgent使用了两个vision encoders去分别对高分辨率图片和低分辩率图片去进行编码，在得到高分辨率特征和低分辨率特征后，使用cross- attention进行高低分辨率特征融合。
- **图片切片方法**（patch-division method）：将高分辨率图像分割成多个小块，再利用低分辨率编码器去执行。其中图片切片方法可能需要positional embedding去提示每个小块在原始图片中的相对位置。

在众多因素当中，图片分辨率的重要性要大于模型参数量和和训练数据组成。

### 预训练LLM

与从头开始训练LLM相比，从预先训练好的LLM开始更有效和实用。通过对web语料库进行大量的预训练，llm嵌入了丰富的世界知识，并展示了很强的泛化和推理能力。表2中总结了常用的和公开使用的llm。大多数llm在GPT-3之后，属于因果解码器类别

<img src="多模态信息处理\image-20240912205411017.png" alt="image-20240912205411017" style="zoom:80%;" />

近年来，对混合专家模型（MoE）体系结构的探索越来越引起了[65]、[66]、[67]的关注。与密集模型相比，稀疏体系结构通过选择性地激活参数，可以在不增加计算成本的情况下放大总参数的大小。根据经验，MM1和MoE-LLaVA发现，MoE实现在几乎所有的基准测试上都比密集的实现获得了更好的性能。

### 模态接口

由于llm只能感知文本，因此弥合自然语言和其他模式之间的差距是必要的。然而，以端到端方式训练一个大型多模式模型将是昂贵的。一种更实用的方法是在预先训练过的视觉编码器和LLM之间引入一个可学习的连接器。另一种方法是在专家模型的帮助下将图像翻译成语言，然后将该语言发送到LLM。

#### 可学习的连接器

它负责弥合不同模式之间的差距。具体来说，该模块将信息投射到LLM能够有效理解的空间中。基于多模态信息的融合方式，大致有两种实现这种接口的方法，即token级融合和feature级融合。

##### token级融合

对于在token层面的融合，modality encoder输出的特征会被转化成与LLM输入等价的token representation，通过将text token representation和其他模态转化后的token representation进行拼接，可以作为“可以被理解”的LLM输入。

* **BLIP-2方案**：利用一组可学习的查询token，以基于查询的方式提取信息。这种q-formers方法将视觉token压缩为更少的表示向量。

* 

# 参考

吴友政,李浩然,姚霆,等.多模态信息处理前沿综述：应用、融合和预训练[J].中文信息学报,2022,36(05):1-20.

A Survey on Multimodal Large Language Models